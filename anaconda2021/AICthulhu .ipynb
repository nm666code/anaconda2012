{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c5cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我收集的Lovecraft小說共有 357538 中文字\n",
      "包含了 3363 個獨一無二的字\n"
     ]
    }
   ],
   "source": [
    "file = open(\"Cthulhu.txt\", mode='r',encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "n = len(text)\n",
    "num_words = len(set(text))\n",
    "print(f\"我收集的Lovecraft小說共有 {n} 中文字\")\n",
    "print(f\"包含了 {num_words} 個獨一無二的字\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711f1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 初始化一個以字為單位的 Tokenizer\n",
    "tokenizer = tf.keras\\\n",
    "    .preprocessing\\\n",
    "    .text\\\n",
    "    .Tokenizer(\n",
    "        num_words=num_words,\n",
    "        char_level=True,\n",
    "        filters=''\n",
    ")\n",
    "\n",
    "# 讓 tokenizer 讀過全文，\n",
    "# 將每個新出現的字加入字典並將中文字轉\n",
    "# 成對應的數字索引\n",
    "tokenizer.fit_on_texts(text)\n",
    "text_as_int = tokenizer\\\n",
    "    .texts_to_sequences([text])[0]\n",
    "\n",
    "# # 隨機選取一個片段文本方便之後做說明\n",
    "# s_idx = 21004\n",
    "# e_idx = 21020\n",
    "# partial_indices = \\\n",
    "#     text_as_int[s_idx:e_idx]\n",
    "# partial_texts = [\n",
    "#     tokenizer.index_word[idx] \\\n",
    "#     for idx in partial_indices\n",
    "# ]\n",
    "\n",
    "# # 渲染結果，可忽略\n",
    "# print(\"原本的中文字序列：\")\n",
    "# print()\n",
    "# print(partial_texts)\n",
    "# print()\n",
    "# print(\"-\" * 20)\n",
    "# print()\n",
    "# print(\"轉換後的索引序列：\")\n",
    "# print()\n",
    "# print(partial_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e541762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _type = type(text_as_int)\n",
    "# n = len(text_as_int)\n",
    "# print(f\"text_as_int 是一個 {_type}\\n\")\n",
    "# print(f\"小說的序列長度： {n}\\n\")\n",
    "# print(\"前 5 索引：\", text_as_int[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d86f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"實際丟給模型的數字序列：\")\n",
    "# print(partial_indices[:-1])\n",
    "# print()\n",
    "# print(\"方便我們理解的文本序列：\")\n",
    "# print(partial_texts[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265ce9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方便說明，實際上我們會用更大的值來\n",
    "# 讓模型從更長的序列預測下個中文字\n",
    "SEQ_LENGTH = 10  # 數字序列長度\n",
    "BATCH_SIZE = 128 # 幾筆成對輸入/輸出\n",
    "\n",
    "# text_as_int 是一個 python list\n",
    "# 我們利用 from_tensor_slices 將其\n",
    "# 轉變成 TensorFlow 最愛的 Tensor <3\n",
    "characters = tf\\\n",
    "    .data\\\n",
    "    .Dataset\\\n",
    "    .from_tensor_slices(\n",
    "        text_as_int)\n",
    "\n",
    "# 將被以數字序列表示的文本\n",
    "# 拆成多個長度為 SEQ_LENGTH (10) 的序列\n",
    "# 並將最後長度不滿 SEQ_LENGTH 的序列捨去\n",
    "sequences = characters\\\n",
    "    .batch(SEQ_LENGTH + 1, \n",
    "           drop_remainder=True)\n",
    "\n",
    "#全文所包含的成對輸入/輸出的數量\n",
    "steps_per_epoch = \\\n",
    "    len(text_as_int) // SEQ_LENGTH\n",
    "\n",
    "# 這個函式專門負責把一個序列\n",
    "# 拆成兩個序列，分別代表輸入與輸出\n",
    "# （下段有 vis 解釋這在做什麼）\n",
    "def build_seq_pairs(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# 將每個從文本擷取出來的序列套用上面\n",
    "# 定義的函式，拆成兩個數字序列\n",
    "# 作為輸入／輸出序列\n",
    "# 再將得到的所有數據隨機打亂順序\n",
    "# 最後再一次拿出 BATCH_SIZE（128）筆數據\n",
    "# 作為模型一次訓練步驟的所使用的資料\n",
    "ds = sequences\\\n",
    "    .map(build_seq_pairs)\\\n",
    "    .shuffle(steps_per_epoch)\\\n",
    "    .batch(BATCH_SIZE, \n",
    "           drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48da40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 512)          1721856   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (128, None, 1024)         6295552   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 3363)         3447075   \n",
      "=================================================================\n",
      "Total params: 11,464,483\n",
      "Trainable params: 11,464,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "EMBEDDING_DIM = 512\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# 使用 keras 建立一個非常簡單的 LSTM 模型\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# 詞嵌入層\n",
    "# 將每個索引數字對應到一個高維空間的向量\n",
    "model.add(\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=num_words, \n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        batch_input_shape=[\n",
    "            BATCH_SIZE, None]\n",
    "))\n",
    "\n",
    "# LSTM 層\n",
    "# 負責將序列數據依序讀入並做處理\n",
    "model.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "    units=RNN_UNITS, \n",
    "    return_sequences=True, \n",
    "    stateful=True, \n",
    "    recurrent_initializer='glorot_uniform'\n",
    "))\n",
    "\n",
    "# 全連接層\n",
    "# 負責 model 每個中文字出現的可能性\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        num_words))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c7fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數，決定模型一次要更新的步伐有多大\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# 定義模型預測結果跟正確解答之間的差異\n",
    "# 因為全連接層沒使用 activation func\n",
    "# from_logits= True \n",
    "def loss(y_true, y_pred):\n",
    "    return tf.keras.losses\\\n",
    "    .sparse_categorical_crossentropy(\n",
    "        y_true, y_pred, from_logits=True)\n",
    "\n",
    "# 編譯模型，使用 Adam Optimizer 來最小化\n",
    "# 剛剛定義的損失函數\n",
    "model.compile(\n",
    "    optimizer=tf.keras\\\n",
    "        .optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE), \n",
    "    loss=loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257763e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 10 # 決定看幾篇文本\n",
    "# history = model.fit(\n",
    "#     ds, # 前面使用 tf.data 建構的資料集\n",
    "#     epochs=EPOCHS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b36e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "callbacks = [\n",
    "    tf.keras.callbacks\\\n",
    "        .TensorBoard(\"logs\"),\n",
    "    # 你可以加入其他 callbacks 如\n",
    "    # ModelCheckpoint,\n",
    "    # EarlyStopping\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    ds,\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks\n",
    ")\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e00c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14236), started 20:40:10 ago. (Use '!kill 14236' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a67cb8dfb457a93f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a67cb8dfb457a93f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8243e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟訓練時一樣的超參數，\n",
    "# 只差在 BATCH_SIZE 為 1\n",
    "EMBEDDING_DIM = 512\n",
    "RNN_UNITS = 1024\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# 專門用來做生成的模型\n",
    "infer_model = tf.keras.Sequential()\n",
    "\n",
    "# 詞嵌入層\n",
    "infer_model.add(\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=num_words, \n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        batch_input_shape=[\n",
    "            BATCH_SIZE, None]\n",
    "))\n",
    "\n",
    "# LSTM 層\n",
    "infer_model.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "    units=RNN_UNITS, \n",
    "    return_sequences=True, \n",
    "    stateful=True\n",
    "))\n",
    "\n",
    "# 全連接層\n",
    "infer_model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        num_words))\n",
    "\n",
    "# 讀入之前訓練時儲存下來的權重\n",
    "infer_model.load_weights(\"model.h5\")\n",
    "infer_model.build(\n",
    "    tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5ab0c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_indices = [104, 349, 395, 294, 57, 16, 4, 82, 1, 776, 6, 487, 390, 22, 297, 241]\n",
    "\n",
    "# 增加 batch 維度丟入模型取得預測結果後\n",
    "# 再度降維，拿掉 batch 維度\n",
    "input = tf.expand_dims(\n",
    "    seed_indices, axis=0)\n",
    "predictions = infer_model(input)\n",
    "predictions = tf.squeeze(\n",
    "    predictions, 0)\n",
    "\n",
    "# 利用生成溫度影響抽樣結果\n",
    "temperature = 0.7\n",
    "predictions /= temperature\n",
    "\n",
    "# 從 4330 個分類值中做抽樣\n",
    "# 取得這個時間點模型生成的中文字\n",
    "sampled_indices = tf.random\\\n",
    "    .categorical(\n",
    "        predictions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74d7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[104 349 395 294  57  16   4  82   1 776   6 487 390  22 297 241]], shape=(1, 16), dtype=int32)\n",
      "tf.Tensor([[208]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[104 349 395 294  57  16   4  82   1 776   6 487 390  22 297 241 208]], shape=(1, 17), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "start_seed = ['最', '終', '結', '果', '都', '是', '一', '樣', '的', '：', '在', '混', '亂', '到', '達', '頂']\n",
    "input_eval = [tokenizer.word_index[s]\\\n",
    "                  for s in start_seed]\n",
    "input_eval = tf.expand_dims(input_eval, axis=0)\n",
    "print(input_eval)\n",
    "pridection_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "pridection_id = tf.expand_dims([pridection_id], axis=0)\n",
    "pridection_id = tf.cast(pridection_id,tf.int32)\n",
    "print(pridection_id)\n",
    "print(tf.concat([input_eval,pridection_id],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5965ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "但最令他印象深刻的還是由密集的尖頂與高塔所組成的那副讓人不知所措的景象。這些尖塔聳立在遙遠的平原上。事後，只有一陣陣回音還在哀怨地念叨著那些長著威脅著外部世界北方大陸的海岸線，至於他們到底坦白了一樣，將之前的建築風格與要冒險穿過更多的街道，而且這條狗一定會在它們的幫助下找到了吉爾曼。他們已經意識到在修築這些東西的大部分皮膚都鮮明的展示在了二副那些生活在山巔之上、更加幽暗的街道上，而興奮的商人們進行\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model,start_seed,gen_size=20,temp=0.8):\n",
    "    input_eval = [tokenizer.word_index[s]\\\n",
    "                  for s in start_seed]\n",
    "    input_eval = tf.expand_dims(input_eval, axis=0)\n",
    "    model.reset_states()\n",
    "    for i in range(gen_size):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions /= temp\n",
    "        pridection_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        pridection_id = tf.expand_dims([pridection_id], axis=0)\n",
    "        pridection_id = tf.cast(pridection_id,tf.int32)\n",
    "        input_eval = tf.concat([input_eval,pridection_id],1)\n",
    "    output = [tokenizer.index_word[idx]\\\n",
    "                  for idx in input_eval.numpy().flatten()]\n",
    "    return ''.join(output)\n",
    "gen_text = generate_text(infer_model,\"但最令他印象深刻的還是由密集的尖頂與高塔所組成的那副讓人不知所措的景象。這些尖塔聳立在遙遠的平原上。\",150,0.5)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25b0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157a60b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
